<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>RBM</title>
<link href="./style.css" rel="stylesheet">
<!-- <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script> -->
</head>

<body>
<div class="content">
  <h1><strong>RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control</strong></h1>
  <h2 style="text-align:center;">CVPR 2024</h2>
  <p id="authors"><a href="https://amirhertz.github.io/">Amir Hertz<sup>* 1</sup></a> <a href="">Andrey Voynov<sup>* 1</sup></a> <a href="">Shlomi Fruchter<sup>† 1</sup></a> <a href="https://danielcohenor.com/">Daniel Cohen-Or<sup>† 1,2</sup></a><br>

    <span style="font-size: 16px"><br>
        <sup>1</sup> Google Research <sup>2</sup> Tel Aviv University <br>
     <sup>*</sup>Indicates Equal Contribution <sup>†</sup>Indicates Equal Advising</span>
        </p>

  <img src="./data/web1.png" class="teaser-gif" style="width:100%;">
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2312.02133" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	        <a href="https://github.com/google/style-aligned/" target="_blank">[Code]</a>
          </p>
    </font>
    <h3 style="text-align:center;">StyleAligned performs consistent style generation with a pretrained diffusion model <u>without fine-tuning</u>.</h3>
</div>


<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.</p>
</div>


<div class="content">
    <h2>Problem Statement</h2>
    <p> While producing high-quality images that are aligned with any textual description, state-of-the-art text-to-image models often create images that diverge significantly in their interpretations of the same stylistic descriptor. Given a style description of “minimal origami”, standard text-to-image generation (left) outputs images with significantly different styles. Our method makes the model generation style persistent (right).</p>
    <br>
    <img class="summary-img" src="./data/web2.png" style="width:100%;"> <br>
  </div>

<div class="content">
  <h2>Style Aligned Generation</h2>
  <p>Generation of images with a style aligned to the reference image on the left. In each diffusion denoising step, all the images, except the reference, perform a shared self-attention with the reference image.</p>
  <br>
  <img class="summary-img" src="./data/web3.png" style="width:100%;"> <br>
  <br>
  <p>The target images attend to the reference image by applying AdaIN over their queries and keys using the reference queries and keys. Then, we apply shared attention where the target features are updated by both the target values Vt and the reference values Vr.</p>
</div>


<div id="results" class="content">
  <h2>Results</h2>
  <p>Our method enables style-consistent content generation using different prompts <b>without fine-tuning</b>  </p>
  <img class="summary-img" src="./data/web4.png" style="width:100%;">
</div>

<div id="results2" class="content">
  <h2>Integration with Other Methods</h2>
  <p>StyleAligned can be easily combined with other methods.</p>


    <h4><a href="https://github.com/lllyasviel/ControlNet" target="_blank">ControlNet </a> + StyleAligned</h4>

    <img class="summary-img" src="./data/web5.png" style="width:100%;">
</div>


<div class="content">
  <h2>Stylization w/ style descriptions</h2>
  <p>Generation of images with a style aligned to the reference image on the left. In each diffusion denoising step, all the images, except the reference, perform a shared self-attention with the reference image.</p>
  <br>
  <img class="summary-img" src="./data/web6_1.png" style="width:100%;"> <br>
  <br>
  <h2>Stylization w/o style descriptions</h2>
  <p>Generation of images with a style aligned to the reference image on the left. In each diffusion denoising step, all the images, except the reference, perform a shared self-attention with the reference image.</p>
  <br>
  <img class="summary-img" src="./data/web6_2.png" style="width:100%;"> <br>
  <br>

  <p>The target images attend to the reference image by applying AdaIN over their queries and keys using the reference queries and keys. Then, we apply shared attention where the target features are updated by both the target values Vt and the reference values Vr.</p>
</div>




<div class="content">
  <h2>BibTex</h2>
  <code> @article{hertz2023StyleAligned,<br>
  &nbsp;&nbsp;title={Style Aligned Image Generation via Shared Attention},<br>
  &nbsp;&nbsp;author={Hertz, Amir and Voynov, Andrey and Fruchter, Shlomi and Cohen-Or, Daniel},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2312.02133},<br>
  &nbsp;&nbsp;year={2023}<br>
  } </code> 
</div>


<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Or Patashnik, Matan Cohen, Yael Pritch, and Yael Vinker for their valuable inputs and early feedback that contributed to this work. We especially thank Yael Vinker for providing one of her <a href="https://yael-vinker.github.io/website/drawing.html">artworks</a> as a style reference.
  </p>
</div>
</body>


</html>
